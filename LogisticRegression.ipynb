{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yAo1vtMndjuP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47, 3) (47, 1) (3, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv( '/Users/antique/Desktop/lr-data 2.csv', names=['x1','x2','y']) \n",
    "#df.head()\n",
    "X = df.iloc[:, 0:2].values\n",
    "y = df.iloc[:, 2].to_frame().values\n",
    "m = len(X)\n",
    "ones = np.ones((m,1))\n",
    "\n",
    "X = np.hstack((ones, X))\n",
    "alpha = 0.01\n",
    "num_iters = 400\n",
    "theta = np.zeros((X.shape[1],1))\n",
    "\n",
    "\n",
    "print(X.shape, y.shape, theta.shape,)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def cost_function(X, y, theta): #cost J\n",
    "    m = y.size\n",
    "    h = sigmoid(X.dot(theta))\n",
    "\n",
    "    J =(-1/m) * ( np.dot( np.log(h).T , y) + np.dot( np.log(1-h).T, (1-y)) )\n",
    "\n",
    "    return np.inf if np.isnan(J[0]) else J[0]\n",
    "\n",
    "def predict(x, theta):\n",
    "    return sigmoid(x.dot(theta))\n",
    "\n",
    "'''def gradientDescent(x, y, theta, alpha, m, numIterations):\n",
    "    J_history = np.zeros(shape=(numIterations, 1))\n",
    "    xTrans = x.transpose()\n",
    "    for i in range(0, numIterations):\n",
    "        hypothesis = 1.0 / (1.0 + np.exp((-1) *np.dot(x, theta)))\n",
    "        #calculates the hypothesis by multiplying the parameter theta with input and the value is subjected to the sigmoid function\n",
    "        loss = hypothesis - y \n",
    "        #calculats the difference between the hypothesis and the actual value\n",
    "        cost = (-1)*np.sum((y*np.log(hypothesis)) + ((1-y)*np.log(1-hypothesis))) / (m)\n",
    "        #calculates the cost function for logistic regression\n",
    "        gradient = np.dot(xTrans, loss) / m\n",
    "        # update\n",
    "        theta = theta - alpha * gradient\n",
    "        J_history[i][0] = cost\n",
    "    return theta, J_history\n",
    "'''\n",
    "def gradientDescent(X, y, theta, alpha, m = len(y), num_iters = 4000):\n",
    "    m = len(y) # number of training examples\n",
    "    J_history = np.zeros((num_iters, 1))\n",
    "    \n",
    "    for iter in range(num_iters):\n",
    "        h = sigmoid(X.dot(theta))\n",
    "        theta = theta - alpha*(1/m)*(X.T.dot(h-y))\n",
    "        \n",
    "        J_history[iter] = cost_function(X, y, theta)\n",
    "        \n",
    "    return(theta, J_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69314718])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_function( X, y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/antique/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:26: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.26379376],\n",
       "       [ 0.29919751],\n",
       "       [ 0.18741676]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta, jh = gradientDescent(X, y, theta, alpha, len(y), 400)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(np.array([1,45,85]).dot(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LogisticRegression",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
